{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc17c3b-d385-45a6-8f50-733bc05d0eeb",
   "metadata": {},
   "source": [
    "# https://github.com/geekyspartan/Image-segmentation-using-SLIC-superpixels-and-graph-cuts/blob/master/main.py\n",
    "# this is not good enough (need more than a binary mask)\n",
    "\n",
    "import maxflow\n",
    "from scipy.spatial import Delaunay"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "04754bc9-85b8-402c-9af2-3fe5b2f94418",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data\n",
    "\n",
    "[Link to paper](https://arxiv.org/abs/2401.10891)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c6a675-203b-429f-9da1-d216fc68ef81",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b73e8af-31d5-41dd-85e2-931f8ba31431",
   "metadata": {},
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from depth_anything.dpt import DepthAnything\n",
    "from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n",
    "\n",
    "def get_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    return np.array(img)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10e6814-9341-4bc6-b542-9614788675b1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "encoder = 'vits' # can also be 'vitb' or 'vitl'\n",
    "\n",
    "transform = Compose([\n",
    "    Resize(\n",
    "        width=518,\n",
    "        height=518,\n",
    "        resize_target=False,\n",
    "        keep_aspect_ratio=True,\n",
    "        ensure_multiple_of=14,\n",
    "        resize_method='lower_bound',\n",
    "        image_interpolation_method=cv2.INTER_CUBIC,\n",
    "    ),\n",
    "    NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    PrepareForNet(),\n",
    "])\n",
    "\n",
    "depth_anything = DepthAnything.from_pretrained('LiheYoung/depth_anything_{:}14'.format(encoder)).eval()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b86bd665-243d-4e53-89ef-0d969fa8098e",
   "metadata": {},
   "source": [
    "### Image Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5424fa16-13f7-4567-ad81-613d0a1b54a4",
   "metadata": {},
   "source": [
    "# img_url = \"https://koitu.com/gallery/photos/IMG_20190310_185804.jpg\"\n",
    "img_url = \"https://koitu.com/gallery/photos/PXL_20240118_062519789.jpg\"\n",
    "# img_url = \"https://www.prints-online.com/p/164/exposition-universelle-internationale-paris-23439848.jpg\"\n",
    "\n",
    "img = get_image_from_url(img_url)\n",
    "img = img / 255.0  # img values should be in range [0, 1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663f2f57-10c4-43db-b84e-0152b7cab496",
   "metadata": {},
   "source": [
    "%%time\n",
    "image = transform({'image': img})['image']\n",
    "image = torch.from_numpy(image).unsqueeze(0)\n",
    "\n",
    "depth = depth_anything(image)  # depth shape: 1xHxW\n",
    "depth = depth.detach().squeeze()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e623d5c-c41a-42a4-86a2-53386de4f7c6",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Input')\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Predicted Depth')\n",
    "plt.imshow(depth)\n",
    "\n",
    "plt.colorbar(cax = plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8649d23f-1084-4354-a4ab-d632e7f5729d",
   "metadata": {},
   "source": [
    "# np.save(\"kyoto_original.npy\", img)\n",
    "# np.save(\"kyoto_depth.npy\", depth.numpy())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "59e8836c-9acc-4825-9295-e21a6ac4b46d",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04593bd0-40ed-4acc-b3f7-f4a9236d21c2",
   "metadata": {},
   "source": [
    "from sklearn.cluster import KMeans"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08914549-ce3b-4515-bb8f-88c01e5ead42",
   "metadata": {},
   "source": [
    "d = depth.numpy()\n",
    "n, m = np.shape(d)\n",
    "\n",
    "d = np.reshape(d, (n*m, 1))\n",
    "\n",
    "k_means = KMeans(n_clusters=4)\n",
    "k_means.fit(d)\n",
    "res = k_means.predict(d)\n",
    "\n",
    "res = np.reshape(res, (n,m))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c03caa7-8e2e-4286-be84-b947821f1c62",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Input')\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Predicted Depth')\n",
    "plt.imshow(res)\n",
    "\n",
    "plt.colorbar(cax = plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a3439786-0520-4106-b5e9-3ff468b8cc82",
   "metadata": {},
   "source": [
    "These results seem pretty decent however the only information that k-means is using is the depth map so this method might only be good for getting some idea about how many layers would be appropriate for the tunnel book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6b88943-1c34-48f9-971e-c90e1adfd4a8",
   "metadata": {},
   "source": [
    "from skimage.segmentation import slic, mark_boundaries\n",
    "from skimage.transform import resize\n",
    "\n",
    "# https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_segmentations.html\n",
    "# https://scikit-image.org/docs/stable/auto_examples/transform/plot_rescale.html"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c47ac515-77c3-409e-8c05-2580d64ebe24",
   "metadata": {},
   "source": [
    "slic_segments_test = slic(img, n_segments=250, compactness=10, sigma=1, start_label=1)\n",
    "plt.imshow(mark_boundaries(img, slic_segments_test))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f44cff83-7f34-4ed9-bb5b-6b1ed879a0ad",
   "metadata": {},
   "source": [
    "d = depth.numpy()\n",
    "n, m = d.shape\n",
    "n_d = d / np.max(np.reshape(d, n * m))  # normalized to [0,1]\n",
    "\n",
    "rescaled_img = resize(img, (n, m), anti_aliasing=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b71483b9-8104-48c1-b005-b5e6ca726da9",
   "metadata": {},
   "source": [
    "rgb_slic = slic(rescaled_img, n_segments=250, compactness=10, sigma=1, start_label=1)\n",
    "plt.imshow(mark_boundaries(rescaled_img, rgb_slic))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59636616-aff1-4768-aa62-ad057ad00c70",
   "metadata": {},
   "source": [
    "d_slic = slic(n_d, n_segments=500, compactness=0.03, sigma=1, start_label=1, channel_axis=None)\n",
    "plt.imshow(mark_boundaries(rescaled_img, d_slic))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df4371a0-e489-40b9-8642-1825821bacd3",
   "metadata": {},
   "source": [
    "rgbd = np.zeros((n, m, 4))\n",
    "rgbd[:,:,0:3] = rescaled_img\n",
    "rgbd[:,:,3] = n_d\n",
    "\n",
    "rgbd_slic = slic(rgbd, n_segments=500, compactness=0.1, sigma=1, start_label=1)\n",
    "plt.imshow(mark_boundaries(rescaled_img, rgbd_slic))\n",
    "# plt.imshow(segments_slic)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "02c62e9c-3670-419a-a9cd-f22f53bbc936",
   "metadata": {},
   "source": [
    "It does not appear that combining the depth information with the color information has made any real improvements..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1afff840-04bf-4390-8018-98bb1e019bae",
   "metadata": {},
   "source": [
    "segment_ids = np.unique(d_slic)\n",
    "\n",
    "masks = np.array([(d_slic == i) for i in segment_ids])\n",
    "d_avgs = np.zeros((n, m), dtype='float')\n",
    "for mk in masks:\n",
    "    d_avgs[mk] = np.mean(n_d[mk])\n",
    "\n",
    "plt.imshow(d_avgs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "722d4a90-21c3-4717-a08b-038e5f29cebf",
   "metadata": {},
   "source": [
    "d = np.reshape(d_avgs, (n*m, 1))\n",
    "\n",
    "k_means = KMeans(n_clusters=4)\n",
    "k_means.fit(d)\n",
    "d_avgs_kmeans = k_means.predict(d)\n",
    "\n",
    "res = np.reshape(d_avgs_kmeans, (n,m))\n",
    "plt.imshow(res)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a43e4a0c-d3d2-4a8e-a1c7-908d6e6e86fc",
   "metadata": {},
   "source": [
    "layer_ids = np.unique(d_avgs_kmeans)\n",
    "# layer_ids = np.unique(res)\n",
    "img_masks = np.array([(res == i) for i in layer_ids])\n",
    "img_layers = np.zeros((4, n, m, 3))\n",
    "\n",
    "print(img_masks.shape)\n",
    "for i, mk in enumerate(img_masks):\n",
    "    img_layers[i,mk] = rescaled_img[mk]\n",
    "\n",
    "plt.figure(figsize=(15,9))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Layer 0')\n",
    "plt.imshow(img_layers[1,:,:])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('Layer 1')\n",
    "plt.imshow(img_layers[3,:,:])\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('Layer 2')\n",
    "plt.imshow(img_layers[0,:,:])\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title('Layer 3')\n",
    "plt.imshow(img_layers[2,:,:])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b050685b-76af-4fee-a920-5dc6f45bb003",
   "metadata": {},
   "source": [
    "this result is only slightly better than just using k-means directly on the depth map. It does a bit of a better job of keeping blocks of content on the same layer however we are still not using RGB values to segment the image...\n",
    "\n",
    "Some things to consider:\n",
    "- bright objects like the torii gate in the center of the image are dominant enough to maybe even deserve its own layer even though the depth estimate does not consider it far enough from the trees\n",
    "- I wonder how well this will perform on things like anime screenshots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74f42dc7-0fdd-4e26-83bf-0019643cb95b",
   "metadata": {},
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0e60ff8-9de2-4833-af8e-0a4720823079",
   "metadata": {},
   "source": [
    "model_path = \"/srv/backup/projects/cs413/Depth-Anything/sam/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=model_path)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1705cf03-81e2-4f01-b35e-be24f33e2b46",
   "metadata": {},
   "source": [
    "masks = mask_generator.generate(rescaled_img)\n",
    "\n",
    "sam_res = np.zeros((n, m))\n",
    "for i, mk in enumerate(masks):\n",
    "    msk = mk['segmentation']\n",
    "    sam_res[msk] = np.mean(n_d[msk])\n",
    "    \n",
    "plt.imshow(sam_res)\n",
    "plt.colorbar(cax = plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76c5f1b5-11c4-4625-879e-43e2aec0fb7a",
   "metadata": {},
   "source": [
    "d = np.reshape(sam_res, (n*m, 1))\n",
    "\n",
    "k_means = KMeans(n_clusters=4)\n",
    "k_means.fit(d)\n",
    "sam_kmeans = np.reshape(k_means.predict(d), (n,m))\n",
    "\n",
    "plt.imshow(sam_kmeans)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06652735-a04b-442c-83fa-ccd10da9bc5f",
   "metadata": {},
   "source": [
    "layer_ids = np.unique(sam_kmeans)\n",
    "# layer_ids = np.unique(res)\n",
    "img_masks = np.array([(sam_kmeans == i) for i in layer_ids])\n",
    "img_layers = np.zeros((4, n, m, 3))\n",
    "\n",
    "print(img_masks.shape)\n",
    "for i, mk in enumerate(img_masks):\n",
    "    img_layers[i,mk] = rescaled_img[mk]\n",
    "\n",
    "plt.figure(figsize=(15,9))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Layer 0')\n",
    "plt.imshow(img_layers[1,:,:])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('Layer 1')\n",
    "plt.imshow(img_layers[3,:,:])\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('Layer 2')\n",
    "plt.imshow(img_layers[0,:,:])\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title('Layer 3')\n",
    "plt.imshow(img_layers[2,:,:])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174299a1",
   "metadata": {},
   "source": [
    "# https://github.com/geekyspartan/Image-segmentation-using-SLIC-superpixels-and-graph-cuts/blob/master/main.py\n",
    "# this is not good enough (need more than a binary mask)\n",
    "\n",
    "import maxflow\n",
    "from scipy.spatial import Delaunay"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "00c2cf2a-5d1a-40b2-88d6-013dc9f3a92f",
   "metadata": {},
   "source": [
    "# rgb_slic = slic(rescaled_img, n_segments=250, compactness=10, sigma=1, start_label=1)\n",
    "rgb_slic = slic(rescaled_img, n_segments=250, compactness=18.5, sigma=1, start_label=1)\n",
    "segments_ids = np.unique(rgb_slic)\n",
    "\n",
    "# centers\n",
    "centers = np.array([np.mean(np.nonzero(rgb_slic==i),axis=1) for i in segments_ids])\n",
    "print(np.shape(centers))\n",
    "\n",
    "# neighbors via Delaunay tesselation\n",
    "tri = Delaunay(centers)\n",
    "\n",
    "indptr, indices = tri.vertex_neighbor_vertices\n",
    "\n",
    "plt.imshow(rgb_slic)\n",
    "plt.plot(centers[:,1], centers[:,0], '.')\n",
    "\n",
    "i = 0\n",
    "for k in range(len(indptr) - 1):\n",
    "    neigh = indices[indptr[k]:indptr[k+1]]\n",
    "    y1, x1 = centers[k]\n",
    "    \n",
    "    for n in neigh:\n",
    "        y2, x2 = centers[n]\n",
    "        plt.plot((x1, x2), (y1, y2))\n",
    "\n",
    "### TODO: add padding\n",
    "### TODO: css parallax effect"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (depth-anything)",
   "language": "python",
   "name": "depth-anything"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
